\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espaÃ±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=3cm]{geometry}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{empheq}
\usepackage{framed}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{reg}{Rule}
\newtheorem{exer}{Exercise}
\newtheorem{note}{Note}
\usepackage{mathrsfs}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\renewcommand\qedsymbol{QED}

\begin{document}

%Change this for headings
\setcounter{section}{1}
\title{Week 2 Class Notes}

\thispagestyle{empty}

\begin{center}
{\LARGE \bf Class Notes| Week 2}\\
{\large MSO: Introduction to Probability Theory}\\
Fall 2024
\end{center}
%Heading Ends

\tableofcontents

\section{Conditional Probability and Functions on Sample Spaces}

\subsection{Bayes Theorem}

\begin{defn}[Conditional Probability]
Let $(\Omega, \mathcal{f}, \mathbb{P})$ be the probability space and let $A$ be an event such that $\mathbb{P}(A) > 0$. For any event $B$, we define:
\begin{equation}
\mathbb{P}(B|A) = \frac{\mathbb{P}(A \bigcap B)}{\mathbb{P}(A)}
\end{equation}

to be the conditional probability of $B$ given $A$ has already occured. 

\end{defn}

\begin{prop}
$(\Omega, \mathcal{F}, \mathbb{P}(\cdotp | A))$ is a Probability Space
\end{prop}

\begin{proof}
$$
\mathbb{P}(\Omega|A) = \frac{\mathbb{P}(\Omega \bigcap A)}{\mathbb{P}(A)} = \frac{\mathbb{P}(A)}{\mathbb{P}(A)} = 1
$$
\\
$$
\mathbb{P}(B|A) = \frac{\mathbb{P}(B \bigcap A)}{\mathbb{P}(A)} > 0
$$
\\
Now we establish countable additivity.\\
If $\{E_n\}_n$ is a sequence of mutually exclusive events, then
so are $\{E_n \bigcap A\}_n$. Now apply the countable additivity of mutually exclusive events on these events to get the result.
\end{proof}

\begin{prop}[\textbf{Multiplication Rule}]
Let $(E_1,E_2,E_3,...,E_n)$ be events in a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ such that $\mathbb{P}(E_1 \cap E_2 \cap E_3 \cap ... \cap E_n) \neq 0$, then:

\begin{equation}
\mathbb{P}(E_1 \cap E_2 \cap E_3 \cap ... \cap E_n) = \mathbb{P}(E_1) \cdotp \mathbb{P}(E_2 | E_1) \cdotp \mathbb{P}(E_3 | E_1 \cap E_2) \cdotp ...  \cdotp \mathbb{P}(E_n | \bigcap_{i = 1}^{n-1} E_i)
\end{equation}

\end{prop}

\begin{proof}
\begin{align*}
\mathbb{P}(E_1 \cap E_2 \cap E_3 \cap ... \cap E_n) & = \mathbb{P}(E_n | \bigcap_{i = 1}^{n-1} E_i) \cdotp \mathbb{P}(\bigcap_{i = 1}^{n-1} E_i) \\
& = \mathbb{P}(E_n | \bigcap_{i = 1}^{n-1} E_i) \mathbb{P}(E_{n-1} | \bigcap_{i = 1}^{n-2} E_i) \cdotp \mathbb{P}(\bigcap_{i = 1}^{n-2} E_i) \\
& = ... \\
& = \mathbb{P}(E_1) \cdotp \mathbb{P}(E_2 | E_1) \cdotp \mathbb{P}(E_3 | E_1 \cap E_2) \cdotp ...  \cdotp \mathbb{P}(E_n | \bigcap_{i = 1}^{n-1} E_i)
\end{align*}
\end{proof}

\begin{defn}[\textbf{Exhaustive Events}]
Let $\mathcal{I}$ be an Indexing Set. The collection of events {$E_i | i \in \mathcal{I}$} is said to be exhaustive if $\bigcup_{i \in \mathcal{I}} = \Omega$
\end{defn}

\begin{theorem}[\textbf{Theorem of Total Probability}]
Let $\mathcal{I}$ be a finite or countably infinite indexing set. Let the events {$E_i | i \in \mathcal{I}$} be mutually exclusive and exhaustive. Then for any event $E$:
$$
\mathbb{P}(E) = \sum_{i \in \mathcal{I}} \mathbb{P}(E \cap E_i) = \sum_{i \in \mathcal{I}} \mathbb{P}(E | E_i) \cdotp \mathbb{P}(E_i) 
$$
\end{theorem}

\begin{theorem}[\textbf{Bayes Theorem}]
Let $\mathcal{I}$ be a finite or countable infinite Indexing Set. Let {$E_i | i \in \mathcal{I}$} be a collection of mutually exclusiev exhaustive events in a probability space $\{\Omega, \mathcal{F}, \mathbb{P}\}$, such that $\mathbb{P}(E_i) > 0$, $\forall i$. Then for any event $E \in \mathcal{F}$ with $\mathbb{P}(E)>0$, we have:

$$
\mathbb{P}(E_j | E) = \frac{\mathbb{P}(E_j)\cdotp\mathbb{P}(E|E_j)}{\sum_{i \in \mathcal{I}} \mathbb{P}(E | E_i) \cdotp \mathbb{P}(E_i) }
$$
\end{theorem}

\begin{defn}[Prior and Posterior Probabilities]
In context of Bayes Theorem, $\mathbb{P}(E_i)$ shall be referred to as prior probabilities and  $\mathbb{P}(E_i | E)$ as posterior probabilities.
\end{defn}


\subsection{Independence of Events}
\begin{defn}[Independence of Two Events]
Two events belonging to the same probability space $\{\Omega, \mathcal{F}, \mathbb{P}\}$ are said to be independent if:
$$
\mathbb{P}(A \cap B) = \mathbb{P}(A)\cdotp \mathbb{P}(A) 
$$
\end{defn}

\begin{note} [\textit{Independence is different from Mutual Exclusiveness}]
If $A$ and $B$ are disjoint (or mutually exclusive), then $\mathbb{P}(A\cap B) = 0$, but independence implies $\mathbb{P}(A\cap B) = \mathbb{P}(A)\cdotp \mathbb{P}(B) $
\end{note}

\begin{defn}[Mutual Independence of a Collection of Events]
\textbf{(a)} Let $\{E_1,E_2,...,E_n\}$ be a finite collection of events in the probability space $\{ \Omega, \mathcal{F}, \mathbb{P} \}$. We say that the collection of events are mutually exclusive, if for all $k \in \{1,2...,n\}$ and indices $1 \leq i_1 < i_2 < ... < i_k \leq n$, we have:

$$
\mathbb{P}(\bigcap_{j = 1}^k E_{i_j}) = \prod_{j = 1}^{k} \mathbb{P}(E_{i_j})
$$

\textbf{(b)} Let $\mathcal{I}$ be an indexing set and let $\{E_i | i \in \mathcal{I}$ be a collection of events in a probability space $\{\Omega, \mathcal{F}, \mathbb{P} \}$. We say that this collection of events is mutually independent or equivalently, the events are mutually independent, if all finite subcollections $\{E_{i_1},E_{i_2}, ... ,E_{i_k} \}$ are mutually independent.
\end{defn}

\begin{note} \textbf{(a)} To check if a collection of $n$ events are mutually independent, we need to check for $\binom{n}{1} + \binom{n}{2} + ... + \binom{n}{n} = 2^n - n -1$ conditions.\\
\textbf{(b)} To check if a collection of events are pairwise independent, we need to check for $\binom{n}{2}$ conditions.
\end{note}

\subsection{Functions defined on Sample Spaces}

\begin{defn}[Functions defined on Sample Spaces]
Sometimes, we assign numerical value to each event in a sample space $\Omega$ of some random experiment $\mathcal{E}$. The domain of this function $X$ is the sample space $\Omega$ and the range is a continuous or discrete set of numbers.  
\end{defn}


\begin{defn}[Pre-Image of a set under a function]
The $\Omega$ be some non-empty set and let $X : \Omega \rightarrow \mathbb{R}$ be a function on this set. Given any subset $A$ of $\mathbb{R}$, we consider the subset $X^{-1}(A)$ of $\Omega$ defined by:
$$
X^{-1}(A) := \{\omega \in \Omega : X(\omega) \in A\}
$$
Then the set $X^{-1}(A)$ shall be referred to as the pre-image of $A$ under the function $X$.
\end{defn}
This means, if $A$ is the range of the function $X$, then the set defined by $X^{-1}(A)$ is the pre-image of $A$ under the function.



\end{document}